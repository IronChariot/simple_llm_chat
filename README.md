# simple_llm_chat
Very simple, minimalist LLM chat interface to run locally, currently just with ollama.
